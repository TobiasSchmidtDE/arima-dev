#-------------------------------------------------------------
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#
#-------------------------------------------------------------


# Arguments
# 1. "X": Path to the one column time series
# 2. "weights_src": The path to weights file / arima model generated by arima training script
# 3. "css destination":
# 4. "p": non-seasonal AR order
# 5. "d":
# 6. "q":
# 7. optional "residuals destination":
# 8. optional "use hdfs": set to true if files are on hdfs and not to be found on local file system


library(Matrix)

start_time  <- Sys.time()

args = commandArgs(trailingOnly=TRUE)

#print(args)
if (length(args) < 10) {
  stop("At least 6 arguments need to be provided. Please check the argument list.", call.=FALSE)
}


XPath = args[1]
weightsPath = args[2]
cssoutputPath = args[3]
p= as.numeric(args[4])
d= as.numeric(args[5])
q= as.numeric(args[6])
residuals_outputPath = args[7]
usehdfs = args[8]

if (is.na(usehdfs)){
  usehdfs = FALSE
}

tryCatch({
  if (usehdfs == TRUE){
    print("usehdfs is on")
    tmp.Xfile <- sprintf("tmp_hadoop_X_%s.mtx", as.numeric(Sys.time()))
    tmp.Wfile  <- sprintf("tmp_hadoop_W_%s.mtx", as.numeric(Sys.time()))
    cmdXfromHDFS <- sprintf("%s dfs -cat %s | perl -pe 's/\t/,/g' > %s", "hdfs", XPath, tmp.Xfile)
    cmdWeightsfromHDFS <- sprintf("%s dfs -cat %s | perl -pe 's/\t/,/g' > %s", "hdfs", weightsPath, tmp.Wfile)
    XPath = tmp.Xfile
    weightsPath = tmp.Wfile
    system(command = cmdXfromHDFS)
    system(command = cmdWeightsfromHDFS)
  }

  arima_css_start_time <- Sys.time()

  data = as.matrix(readMM(XPath))
  weights = as.matrix(readMM(weightsPath))
}, warning = function(w) {
    print(w)
}, error = function(e) {
    print(e)
}, finally = {
  if (usehdfs == TRUE){
    cfile.remove(tmp.Xfile)
    file.remove(tmp.Wfile)
  }
})

fixedmodel = arima(data, fixed = weights, order = c(p,d,q), seasonal = list(order = c(0, 0, 0), period = 0), include.mean = FALSE, transform.pars = FALSE, method = c("CSS"))
sumofsquares = fixedmodel$sigma2
css = 0.5 * log(sumofsquares)


if (cssoutputPath != ""){
  write.table(css, file = cssoutputPath, sep=",", row.names=FALSE, col.names= FALSE)
}

if (!is.na(residuals_outputPath)){
  writeMM (as(matrix(fixedmodel$residuals, nrow = NROW(fixedmodel$residuals), ncol = 1), "CsparseMatrix"),residuals_outputPath)
}

arima_css_end_time <- Sys.time()

if (usehdfs == TRUE){
  copyToLocal = "hdfs dfs -copyFromLocal -f "
  if (cssoutputPath != ""){
    cmdToHDFS <- sprintf("%s %s %s", copyToLocal, cssoutputPath, cssoutputPath)
    system(command = cmdToHDFS)
    file.remove(cssoutputPath)
  }
  if (!is.na(residuals_outputPath)){
  cmdToHDFS <- sprintf("%s %s %s", copyToLocal, residuals_outputPath, residuals_outputPath)
  system(command = cmdToHDFS)
  file.remove(residuals_outputPath)
  }
}

end_time <- Sys.time()

sprintf("execution_time:  %f", (arima_css_end_time - arima_css_start_time))
sprintf("run_time:  %f", (end_time - start_time))
print(paste("arima_css=", css, sep=" "))
